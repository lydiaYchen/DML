# DML

:warning: **I am still developing the course content. The content will be only be finalized by the end of the first week of the semester.**


This repository contains the materials of the  **Seminar Distributed Machine Learning Systems** - fall 2023  at UniNE . 


##  <a name='Importantlinks'></a>Important links

- [Paper Reading List](PaperList.md)




##  <a name='Coursedescription'></a>Course description

Machine learning systems are often conventionally designed for centralized processing in that they first collect data from distributed sources and then execute algorithms on a single server. Due to the limited scalability of processing large amount of data and the long latency delay, there is a strong demand for a paradigm shift to distributed or decentralized ML systems which execute ML algorithms on multiple and in some cases even geographically dispersed nodes.

The aim of this seminar course is to let students learn how to design and build distributed ML systems via paper reading, presentation, discussion, and project prototyping.&nbsp; We provide a broad overview on the design of the state-of-the-art distributed ML systems, with a strong focus on the scalability, resource efficiency, data requirements, and robustness of the solutions. We will present an array of methodologies and techniques that can efficiently scale ML analysis to a large number of distributed nodes against all operation conditions, e.g., system failures and malicious attacks. The specific course topics are listed below.

The course materials will be based on a mixture of classic and recently published papers. The first 3-4 lectures, the basic concept of distributed machine learning will be covered, followed by three weeks of self-study time, and then student presentations will be given.

[//]: # (For each topic, &nbsp;the basic concepts and technology landscape will be first provided and then two state-of-the art of papers will be presented and discussed by students.)


##  <a name='Paper List'></a>Paper


Paper reading and reviews are the key activities in this course. To pass the course, you need to 
- Submit at least 5 reviewers on 5 different topics out of 7 topics, and  
- Present at least one paper out of 7 topics during the second half of the course 


Check [Paper Reading List](PaperList.md)



##  <a name='Courseteam'></a>Course team
This course will be mainly taught by [Prof. Lydia Y Chen]([https://lydiaychen.github.io/]) . The course team is composed of a number of PhDs  who support the course through guest lectures.



Lydia can be alwasy reached at **lydiaychen@ieee.org**. In order to get prompt response about the course, put the email title starting with [DML23]

[//]: # (5. <a name='ECs'></a>ECs)

[//]: # (This is a **5 EC course**, with **140 hours** of course work in total. We expect you to spread the load evenly across the 9 course weeks.)

##  <a name='Learningobjectives'></a>Learning objectives
-  To argue and reason about distributed ML from a systems perspective.
- To understand the behavior and tradeoffs of distributed ML in terms of performance and scalability
- To estimate the importance of data inputs via different techniques, i.e., core set and decomposition methods, for distributed ML systems.
- To understand data poison attacks and design defense strategy for distributed ML systems.
- To analyze the state-of-the art federated machine learning systems and design the failure-resilient communication protocols
- To design and implement methods and techniques for making distributed ML systems more efficient.
- 
##  <a name='dart:Gradingpolicy'></a>:dart: Grading policy

There is no grade but pass/fail of the group project.


**All assessment items (reviews and presentation slides) have to be submitted via ILIAS.**

To pass the course, you need to
1. Submmit at least 5 paper reviews (template and requirement can be found  here )
2. Present at least 1 paper (20 minutes plus Q/A)

##  <a name='Detailedschedule'></a>Detailed schedule


**Week**|**Topic**
:-----|:-----
Week 1 | Distributed Machine Learning I |
Week 2 | Distributed Machine Learning II, acceleration
Week 3| Federated Machine Learning I
Week 4| Fedearted Machine Learning II
Week 5| Self study
Week 6| Self study
Week 7| Self study
Week 8| Self study
Week 9| Paper presentation and discussion on topic 1
Week 10|Paper presentation and discussion on topic 2
Week 11| Paper presentation and discussion on topic 3
Week 12| Paper presentation and discussion on topic 4
Week 13| Paper presentation and discussion on topic 5

